<!DOCTYPE html>
<html lang="en">
  <head prefix="og: https://ogp.me/ns#">
    <link href="/assets/favicon/favicon.ico" rel="shortcut icon">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="UTF-8">
    <title>MSU VSR Benchmark Methodology</title>
    <meta property="og:title" content="MSU VSR Benchmark Methodology">
    <meta property="og:image" content="http://localhost:4000/assets/img/benchmarks/vsr/methodology.gif">
    <meta name="description" content="The evaluation methodology of MSU Video Super Resolution Benchmark">
    <meta property="og:description" content="The evaluation methodology of MSU Video Super Resolution Benchmark">
    <meta property="og:url" content="
http://localhost:4000/benchmarks/video-super-resolution-methodology.html
">
    <meta property="og:type" content="website">
    <link href="/assets/css/common.css" rel="stylesheet" type="text/css">
    <script src="/assets/js/interface.js"></script>
    <link rel="stylesheet" type="text/css" href="/assets/css/nav_arrow.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  </head>
  <body>
    <ul class="navbar">
      <li class="navbar-header">
        <a href="/" class="header">
          <img alt="Main page" class="logo" src="/assets/img/logo.svg">
          <h1>Video processing, compression<br>
            and quality research group
            <div>Based in MSU Graphics & Media Laboratory</div>
          </h1>
        </a>
        <a class="menu-toggle-button">
          <div class="icon"></div>
        </a>
      </li>
      <li>
        <a href="/benchmarks/">Benchmarks <img class="dropdown-icon" src="/assets/icons/dropdown.svg"></a>
        <ul class="dropmenu">
          <li><a href="/benchmarks/deblurring.html">MSU Video Deblurring Benchmark 2022</a></li>
          <li><a href="/benchmarks/video-frame-interpolation.html">MSU Video Frame Interpolation Benchmark 2022</a></li>
          <li><a href="/benchmarks/video-upscalers.html">MSU Video Upscalers Benchmark 2022</a></li>
          <li><a href="/benchmarks/inverse-tone-mapping.html">MSU HDR Video Reconstruction Benchmark 2022</a></li>
          <li><a href="/benchmarks/super-resolution-for-video-compression.html">MSU Super-Resolution for Video Compression Benchmark 2022</a></li>
          <li><a href="/benchmarks/no-reference-video-quality-metrics.html">MSU No-Reference Video Quality Metrics Benchmark 2022</a></li>
          <li><a href="/benchmarks/full-reference-video-quality-metrics.html">MSU Full-Reference Video Quality Metrics Benchmark 2022</a></li>
          <li><a href="/benchmarks/aligners.html">MSU Video Alignment and Retrieval Benchmark</a></li>
          <li><a href="/benchmarks/mobile-video-codec-benchmark.html">MSU Mobile Video Codecs Benchmark 2021</a></li>
          <li><a href="/benchmarks/video-super-resolution.html">MSU Video Super-Resolution Benchmark</a></li>
          <li><a href="/benchmarks/shot-boundary-detection.html">MSU Shot Boundary Detection Benchmark 2020</a></li>
          <li><a href="/benchmarks/deinterlacer.html">MSU Deinterlacer Benchmark</a></li>
          <li><a href="https://videomatting.com/" target="_blank">The VideoMatting Project</a></li>
          <li><a href="https://videocompletion.org/" target="_blank">Video Completion</a></li>
        </ul>
      </li>
      <li>
        <a href="/projects/">Projects <img class="dropdown-icon" src="/assets/icons/dropdown.svg"></a>
        <ul class="dropmenu">
          <li><a href="/codecs/">Codecs Comparisons & Optimization</a></li>
          <li><a href="/vqmt/">VQMT</a></li>
          <li><a href="/stereo_quality/">Video Quality Measurement Tool 3D</a></li>
          <li><a href="/datasets/">MSU Datasets Collection</a></li>
          <li><a href="/metrics/">Metrics Research</a></li>
          <li><a href="/video_filters/">Video Filters</a></li>
          <li><a href="/other/">Other Projects</a></li>
        </ul>
      </li>
      <li><a href="/publications/">Publications</a></li>
      <li><a href="/about/">About Us</a></li>
      <li><a href="/contacts/">Contacts</a></li>
    </ul>
    <div class="content">
      <link href="/assets/css/post.css" rel="stylesheet" type="text/css">
      <div class="tiles-width nav-current">
        <a href="/index.html">Main page</a> &mdash;
        <a href="/benchmarks/">MSU Benchmark Collection</a>
      </div>
      <div class="tiles-width markdown article">
        <link rel="stylesheet" href="/assets/css/benchmarks/style.css" />
        <script src="https://code.highcharts.com/highcharts.js"></script>
        <script src="https://code.highcharts.com/modules/exporting.js"></script>
        <script src="https://code.highcharts.com/modules/export-data.js"></script>
        <script src="https://code.highcharts.com/modules/accessibility.js"></script>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
        <script src="https://code.highcharts.com/highcharts-more.js"></script>
        <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.10.22/css/jquery.dataTables.css" />
        <script type="text/javascript" charset="utf8" src="https://cdn.datatables.net/1.10.22/js/jquery.dataTables.js"></script>
        <h1 id="evaluation-methodology-of-msu-video-super-resolution-benchmark-detail-restoration">Evaluation methodology of MSU Video Super-Resolution Benchmark: Detail Restoration</h1>
        <div id="buttons"></div>
        <script>
          __set_menu_buttons([
          ['Home', '/benchmarks/video-super-resolution.html'],
          ['Participants','/benchmarks/video-super-resolution-participants.html'],
          ['Evaluation methodology', '/benchmarks/video-super-resolution-methodology.html'],
          ['How to participate', '/benchmarks/video-super-resolution.html#participate'],
              ['Contact us', '/benchmarks/video-super-resolution.html#contacts']
          ], 'Evaluation methodology')
        </script>
        <div class="current_content">
          <p>You can read the Methodology below or download the presentation in pdf format 
            <a href="https://drive.google.com/file/d/15eQz5nBPGU91nwV7BMWEFnRbiweXqh91/view?usp=sharing">here</a>.<br />
             You also can see it in 
            Google Slides <a href="https://docs.google.com/presentation/d/1_Qgw9g_TyJrDWh6bWTYAh58TuygqWNqr0CpP_4HFBBw/edit?usp=sharing">here</a>.</p>
          <h3 id="problem_definition">Problem definition</h3>
          <p>Super-Resolution is the process of calculating high-resolution samples from their low-resolution counterparts. Trends in quality assessment 
            of upscaled videos and images are favoring estimation of statistical naturalness in combination with fidelity. But restoration fidelity 
            is much more important than statistical naturalness for some tasks. Some VSR methods may produce the wrong digit or an entirely
            different face. Whether a method’s results are trustworthy depends on how well it restores truthful details. Our benchmark is aimed 
            to find the best algorithms for the restoration of real details during Video Super Resolution processing.</p>
          <h3 id="dataset">Dataset</h3>
          <h4 id="content_types">Content types</h4>
          <p>To analyze a VSR model’s ability to restore real details, we built a test 
            stand containing patterns that are difficult for video restoration (Figure 1).</p>
          <div class="center">
            <div>
              <img width="50%" src="/assets/img/benchmarks/vsr/parts.jpg" />
              <p><i>Figure 1. The test-stand of the benchmark.</i></p>
            </div>
          </div>
          <p>To calculate metrics for particular content types
            and to verify how a model works with different inputs,
            we divide each output frame into parts by detecting
            crosses:<br />
            <b>Part 1. “Board”</b> includes a few small objects and photos of human faces*. Our goal is to obtain results for the model operating on textures with
            small details. The striped fabric and balls of
            yarn may produce a Moire pattern (Figure 2).
            Restoration of human faces is important for
            video surveillance.<br />
            <b>Part 2. “QR”</b> comprises multiple QR codes of differing sizes; the aim is to find the size of the
            smallest recognizable one in the model’s output frame. A low-resolution frame may blend
            QR-code patterns, so models may have difficulty restoring them.<br />
            <b>Part 3. “Text”</b> includes two kinds: handwritten and
            typed. Packing all these difficult elements into
            the training dataset is a challenge, so they are
            each new to the model as it attempts to restore
            them.<br />
            <b>Part 4. “Metal paper”</b> contains foil that was vigorously crumpled. It’s an interesting example
            because of the reflections, which change periodically between frames.<br />
            <b>Part 5. “Color lines”</b> is a printed image with numerous
            thin color stripes. This image is difficult because thin lines of similar colors end up mixing in low-resolution frames.<br />
            <b>Part 6. ‘License-plate numbers”</b> consists of a set of
            car license plates of varying sizes from different countries**. This content is important for
            video surveillance and dashcam development<br />
            <b>Part 7. “Noise”</b> includes difficult noise patterns. Models cannot restore real ground-truth noise, and
            each one produces a unique pattern<br />
            <b>Part 8. “Resolution test chart”</b> contains a resolution test chart with
            patterns that are difficult to restore: a set of
            straight and curved lines of differing thicknesses and directions<br />
            *Photos were generated by <sup><a href="#references">[1]</a></sup>.<br />
            **The license-plate numbers are generated randomly and printed on paper.</p>
          <div class="center">
            <div>
              <video autoplay="" loop="" muted="" playsinline="" width="75%">
                <source src="/assets/img/benchmarks/vsr/muar.av1.mp4" type="video/mp4" />
                <source src="/assets/img/benchmarks/vsr/muar.vp9.webm" type="video/webm" />
                <source src="/assets/img/benchmarks/vsr/muar.x264.mp4" type="video/mp4" />
              </video>
              <p><i>Figure 2. Example of a Moire pattern on the “Board”.</i></p>
            </div>
          </div>
          <h4 id="content_types">Motion types</h4>
          <p>The dataset includes three videos with different types of motion:
            <ul>
              <li><b>Hand tremor</b> — video shooting from the fixed point without a tripod (the photographer holds the camera in his hands). Because of the natural tremor of hands, there is a random small motion in frames
              </li>
              <div class="center">
                <img src="/assets/img/benchmarks/vsr/motion1.png" />
                <video autoplay="" loop="" muted="" playsinline="">
                  <source src="/assets/img/benchmarks/vsr/test1.av1.mp4" type="video/mp4" />
                  <source src="/assets/img/benchmarks/vsr/test1.vp9.webm" type="video/webm" />
                  <source src="/assets/img/benchmarks/vsr/test1.x264.mp4" type="video/mp4" />
                </video>
              </div>
              <li><b>Parallel motion</b> — the camera is moving from side to side in parallel with test-stand</li>
              <div class="center">
                <img src="/assets/img/benchmarks/vsr/motion2.png" />
                <video autoplay="" loop="" muted="" playsinline="">
                  <source src="/assets/img/benchmarks/vsr/test2.av1.mp4" type="video/mp4" />
                  <source src="/assets/img/benchmarks/vsr/test2.vp9.webm" type="video/webm" />
                  <source src="/assets/img/benchmarks/vsr/test2.x264.mp4" type="video/mp4" />
                </video>
              </div>
              <li><b>Rotation</b> — the camera is moving from side to side in a half-circle</li>
              <div class="center">
                <img src="/assets/img/benchmarks/vsr/motion3.png" />
                <video autoplay="" loop="" muted="" playsinline="">
                  <source src="/assets/img/benchmarks/vsr/test3.av1.mp4" type="video/mp4" />
                  <source src="/assets/img/benchmarks/vsr/test3.vp9.webm" type="video/webm" />
                  <source src="/assets/img/benchmarks/vsr/test3.x264.mp4" type="video/mp4" />
                </video>
              </div>
            </ul>
          </p>
          <h4 id="technical">Technical characteristics of the camera</h4>
          <p>We captured the dataset using a Canon EOS 7D
            camera. We quickly took a series of 100 photos and
            used them as a video sequence. The shots were from
            a fixed point without a tripod, so the video contains
            a small amount of random motion. We stored the
            video as a sequence of frames in PNG format, converted from JPG. The camera’s settings were:<br />
            ISO – 4000<br />
            aperture – 400<br />
            resolution – 5184x3456
          </p>
          <h4 id="dataset_preparation">Dataset preparation</h4>
          <p>
            <ul>
              <li><b>Source video</b> has a resolution of 5184x3456 and was stored in the sRGB color space. Each video’s length is 100 frames.</li>
              <li><b>Ground-truth</b>. Each video was degraded by bicubic interpolation to generate a GT of resolution 1920x1280. This step is
                essential because many open-source models lack the
                code to process a large frame; processing large frames
                is also time consuming.</li>
              <li>Then <b>input video</b> was degraded from GT in two ways: bicubic interpolation (BI) and Downsamlping after Gaussian Blurring (BD).</li>
            </ul>
          </p>
          <h4 id="noise_input">Noise input</h4>
          <p>To verify how a model works with noisy data, we prepared noise counterparts for each input video. To generate realistic noise, we use python implementation <sup><a href="#references">[2]</a></sup>
            of the noise model proposed in CBDNet by Liu et al <sup><a href="#references">[3]</a></sup>. We need to set two parameters: one for the Poisson part of the noise and another for the Gauss part of 
            the noise.<br />
            To estimate the level of real noise in our camera, we set a camera on a tripod and capture a sequence of 100 frames from a fixed point. Then we average the sequence 
            to estimate a clean image. Thus we gain hundred of real noise examples. Then we chose parameters for generated noise so that the distributions of generated and real 
            noise are similar (see Figure 3). Our parameters choice: sigma_s = 0.001, sigma_c = 0.035.</p>
          <div class="center">
            <div style="width:75%">
              <img src="/assets/img/benchmarks/vsr/noise.jpg" />
              <p><i>Figure 3. The distribution of real and generated noise.</i></p>
            </div>
          </div>
          <p>Finally, we have 12 tests:</p>
          <div class="center">
            <div style="width:90%">
              <img src="/assets/img/benchmarks/vsr/test_names.png" />
            </div>
          </div>
          <h3 id="metrics">Metrics</h3>
          <h4 id="psnr">PSNR</h4>
          <p>PSNR – commonly used metric based on pixels’ similarity. We noticed that a model, trained on one degradation type and tested on another type, can generate 
            frames with a global shift relative to GT (see Figure 4). Thus we checked integer shifts from [-3,3] in both axes and choose the shift with maximal PSNR value. 
            This maximal value is considered as a metric result in our benchmark.</p>
          <div class="center">
            <div style="width:75%">
              <video autoplay="" loop="" muted="" playsinline="">
                <source src="/assets/img/benchmarks/vsr/shift.av1.mp4" type="video/mp4" />
                <source src="/assets/img/benchmarks/vsr/shift.vp9.webm" type="video/webm" />
                <source src="/assets/img/benchmarks/vsr/shift.x264.mp4" type="video/mp4" />
              </video>
              <p><i>Figure 4. On the left: The same crop from the model’s output and GT frame.<br />
                  On the right: PSNR visualization for this crop.</i></p>
            </div>
          </div>
          <p>We chose PSNR-Y because it’s more efficient than PSNR-RGB. Meanwhile, a correlation between these metrics is high. For metric calculation, we use the 
            implementation from skimage.metrics<sup><a href="#references">[4]</a></sup>. A higher metric value indicates better quality. The metric value for GT is infinite.</p>
          <h4 id="ssim">SSIM</h4>
          <p>SSIM – another commonly used metric based on structure similarity. A shift of frames can influence this metric too. Thus we tried to find the optimal 
            shift similarly to PSNR calculation and noticed that optimal shifts for these metrics can differ, but not more than 1 pixel in any axis. Because SSIM 
            has large computational complexity, we decided to find optimal shift not among all shifts, but near with optimal shift for PSNR (in a distance of 1 pixel 
            in any axis). We calculate SSIM on the Y channel of the YUV color space. For metric calculation, we use the implementation from skimage.metrics<sup><a href="#references">[5]</a></sup>. A higher 
            metric value indicates better quality. The metric value for GT is 1.</p>
          <h4 id="erqa">ERQAv1.0</h4>
          <p>ERQAv1.0 (Edge Restoration Quality Assessment, version 1.0) estimates how well a model has restored edges of the high-resolution frame. Firstly, we find
            edges in both output and GT frames. To do it we use OpenCV implementation<sup><a href="#references">[6]</a></sup> of the Canny algorithm<sup><a href="#references">[7]</a></sup>. A threshold for the initial finding of strong edges 
            is set to 200. And a threshold for edge linking is set to 100. These coefficients allow to highlight edges of all subjects even of small sizes but skip lines, 
            which are not important (see Figure 5).</p>
          <div class="center">
            <div style="width:90%">
              <img src="/assets/img/benchmarks/vsr/edge1.jpg" />
              <p><i>Figure 5. An example of edges, highlighted by the chosen algorithm</i></p>
            </div>
          </div>
          <p>Then we compare these edges by using F1-score. To compensate a one-pixel shift of edge, which is not essential for human perception of objects, we consider 
            as true-positive pixels of output’s edges, which are not in edges of GT but are near (on the difference of one pixel) with the edge of GT(see Figure 6). 
            A higher metric value indicates better quality. The metric value for GT is 1.
          </p>
          <div class="center">
            <div style="width:90%">
              <img src="/assets/img/benchmarks/vsr/edge2.jpg" />
              <p><i>Figure 6. Visualization of F1-score, used for edges comparison</i></p>
            </div>
          </div>
          <h4 id="qrcr">QRCRv1.0</h4>
          <p>QRCRv1.0 (QR-Codes Restoration, version 1.0) finds the smallest size (in pixels) of QR-code, which can be detected in output frames of a model. 
            To project metric values on [0,1], we consider a relation of the smallest QRs’ sizes for GT and output frame (see Figure 7). If in the model’s result 
            we can’t detect any QR-code, the metric value is set to 0. A higher metric value indicates better quality. The metric value for GT is 1.
          </p>
          <div class="center">
            <div style="width:60%">
              <img src="/assets/img/benchmarks/vsr/qr.jpg" />
              <p><i>Figure 7. Example of detected crosses in output and GT frame.<br />
                  The metric value for the output frame is 0.65</i></p>
            </div>
          </div>
          <h4 id="crrm">CRRMv1.0</h4>
          <p>CRRMv1.0 (Colorfullness Reduced-Reference Metric, version 1.0) – calculate colorfullness* in both frames and compare them. To calculate colorfullness we 
            use metric, proposed by Hasler et al.<sup><a href="#references">[8]</a></sup>. Comparison of colorfullness levels is performed as a relation between colorfullness in GT frame and output frame. 
            Then to project metric on [0,1] and penalize both increasing and decreasing of colorfullness, we take the absolute difference between 1 and the relation and 
            then subtract it from 1. A higher metric value indicates better quality. The metric value for GT is 1.<br />
            *Colorfulness measures how colorful an image is: if it’s bright and has a lot of different colors.
          </p>
          <h3 id="metrics_acc">Metrics accumulation</h3>
          <p>Because each model can work differently on different content types, we consider metric values not only on full-frame but also on parts with different content.
            To do this we detect crosses in frames and calculate coordinates of all parts from them.<br />
            Crosses in some frames are distorted and cannot be detected. Thus we choose keyframes, where we can detect all crosses and calculate metrics only on these 
            keyframes. We noticed that metrics values on these keyframes are highly correlated and choose the mean of values through keyframes as a final metric value 
            for each test case.</p>
          <h3 id="subjective">Subjective comparison</h3>
          <p>We cut the sequences to 30 frames and converted them
            to 8 frames per second (fps). This length allows subjects to easily consider details and decide which video
            is better. We then cropped from each video 10 snippets that cover the most difficult patterns for restora-
            tion and conducted a side-by-side pairwise subjective
            evaluation using the Subjectify.us service, which enables crowd-sourced comparisons.</p>
          <p>To estimate information fidelity, <b>we asked participants</b> in the subjective comparison to avoid choosing
            the most beautiful video, but instead <b>choose the one
              that shows better detail restoration</b>. Participants are
            not experts in this field thus they do not have professional biases. Each participant was shown 25 paired
            videos and in each case had to choose the best video
            (“indistinguishable” was also an option). Each pair of
            snippets was shown to 10-15 participants until confidence interval stops changing. Three of pairs for
            each participant are for verification, so the final results exclude their answers. All other responses* from
            1400 successful participants are used to predict subjective scores using the Bradley-Terry.
            *Answers to verification questions are not included in the final result.</p>
          <h3 id="fps">The computational complexity of models</h3>
          <p>We tested each model using NVIDIA Titan RTX and measured runtime on the same test sequence:
            <ul>
              <li>Test case — parallel motion + BD degradation + with noise</li>
              <li>100 frames</li>
              <li>Input resolution — 480×320</li>
            </ul>
            FPS is calculated as the execution time of a full model runtime divided by the number of sequence frames.
          </p>
          <h3 id="references">References</h3>
          <ol>
            <li><a href="https://thispersondoesnotexist.com">https://thispersondoesnotexist.com</a></li>
            <li><a href="https://github.com/yzhouas/CBDNet_ISP">https://github.com/yzhouas/CBDNet_ISP</a></li>
            <li>Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo and Lei Zhang, "Toward Convolutional Blind Denoising of Real Photographs," 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 1712-1722, doi: 10.1109/CVPR.2019.00181.</li>
            <li><a href="https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.peak_signal_noise_ratio">https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.peak_signal_noise_ratio</a></li>
            <li><a href="https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.structural_similarity">https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.structural_similarity</a></li>
            <li><a href="https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga04723e007ed888ddf11d9ba04e2232de">https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga04723e007ed888ddf11d9ba04e2232de</a></li>
            <li><a href="https://en.wikipedia.org/wiki/Canny_edge_detector">https://en.wikipedia.org/wiki/Canny_edge_detector</a></li>
            <li>David Hasler and Sabine Suesstrunk, "Measuring Colourfulness in Natural Images," Proceedings of SPIE - The International Society for Optical Engineering, 2003, volume 5007, pp. 87-95, doi: 10.1117/12.477378.</li>
          </ol>
        </div>
      </div>
      <div class="tiles-width meta">
        <div class="share">
          <link href="/assets/css/sharing-buttons.css" rel="stylesheet" type="text/css">
          <!-- Sharingbutton Twitter -->
          <a class="resp-sharing-button__link"
   href="https://twitter.com/intent/tweet/?text=MSU VSR Benchmark Methodology&amp;url=
http://localhost:4000/benchmarks/video-super-resolution-methodology.html
"
   target="_blank" rel="noopener" aria-label="">
            <div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small">
              <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5 0-4.55 2.04-4.55 4.54 0 .36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3 0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35 0 12.92-6.92 12.92-12.93 0-.2 0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"/>
              </svg>
            </div>
          </a>
          <!-- Sharingbutton Telegram -->
          <a aria-label=""
   class="resp-sharing-button__link"
   href="https://telegram.me/share/url?text=MSU VSR Benchmark Methodology&amp;url=
http://localhost:4000/benchmarks/video-super-resolution-methodology.html
" rel="noopener" target="_blank">
            <div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--small">
              <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M.707 8.475C.275 8.64 0 9.508 0 9.508s.284.867.718 1.03l5.09 1.897 1.986 6.38a1.102 1.102 0 0 0 1.75.527l2.96-2.41a.405.405 0 0 1 .494-.013l5.34 3.87a1.1 1.1 0 0 0 1.046.135 1.1 1.1 0 0 0 .682-.803l3.91-18.795A1.102 1.102 0 0 0 22.5.075L.706 8.475z"/>
              </svg>
            </div>
          </a>
          <!-- Sharingbutton Facebook -->
          <a aria-label=""
   class="resp-sharing-button__link"
   href="https://facebook.com/sharer/sharer.php?u=
http://localhost:4000/benchmarks/video-super-resolution-methodology.html
" rel="noopener"
   target="_blank">
            <div class="resp-sharing-button resp-sharing-button--facebook resp-sharing-button--small">
              <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M18.77 7.46H14.5v-1.9c0-.9.6-1.1 1-1.1h3V.5h-4.33C10.24.5 9.5 3.44 9.5 5.32v2.15h-3v4h3v12h5v-12h3.85l.42-4z"/>
              </svg>
            </div>
          </a>
          <!-- Sharingbutton E-Mail -->
          <a class="resp-sharing-button__link"
   href="mailto:?subject=MSU VSR Benchmark Methodology&amp;body=
http://localhost:4000/benchmarks/video-super-resolution-methodology.html
" target="_self" rel="noopener"
   aria-label="">
            <div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small">
              <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M22 4H2C.9 4 0 4.9 0 6v12c0 1.1.9 2 2 2h20c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zM7.25 14.43l-3.5 2c-.08.05-.17.07-.25.07-.17 0-.34-.1-.43-.25-.14-.24-.06-.55.18-.68l3.5-2c.24-.14.55-.06.68.18.14.24.06.55-.18.68zm4.75.07c-.1 0-.2-.03-.27-.08l-8.5-5.5c-.23-.15-.3-.46-.15-.7.15-.22.46-.3.7-.14L12 13.4l8.23-5.32c.23-.15.54-.08.7.15.14.23.07.54-.16.7l-8.5 5.5c-.08.04-.17.07-.27.07zm8.93 1.75c-.1.16-.26.25-.43.25-.08 0-.17-.02-.25-.07l-3.5-2c-.24-.13-.32-.44-.18-.68s.44-.32.68-.18l3.5 2c.24.13.32.44.18.68z"/>
              </svg>
            </div>
          </a>
          <!-- Sharingbutton LinkedIn -->
          <a class="resp-sharing-button__link"
   href="https://www.linkedin.com/shareArticle?mini=true&amp;url=
http://localhost:4000/benchmarks/video-super-resolution-methodology.html
&amp;title=MSU VSR Benchmark Methodology&amp;summary=MSU VSR Benchmark Methodology&amp;source=
http://localhost:4000/benchmarks/video-super-resolution-methodology.html
"
   target="_blank" rel="noopener" aria-label="">
            <div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small">
              <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M6.5 21.5h-5v-13h5v13zM4 6.5C2.5 6.5 1.5 5.3 1.5 4s1-2.4 2.5-2.4c1.6 0 2.5 1 2.6 2.5 0 1.4-1 2.5-2.6 2.5zm11.5 6c-1 0-2 1-2 2v7h-5v-13h5V10s1.6-1.5 4-1.5c3 0 5 2.2 5 6.3v6.7h-5v-7c0-1-1-2-2-2z"/>
              </svg>
            </div>
          </a>
        </div>
        <span class="date">26 Apr 2021</span>
      </div>
      <div class="tiles-width separator"></div>
      <div class="tiles-width see-also-title">
        See Also
      </div>
      <div class="tiles wide">
        <a class="tile" href="/benchmarks/deblurring.html">
          <div class="image" style="background-image: url(/assets/img/benchmarks/deblurring/preview.png)"></div>
          <div class="title">MSU Video Deblurring Benchmark 2022</div>
          <div class="text">Learn about the best video deblurring methods and choose the best model</div>
        </a>
        <a class="tile" href="/benchmarks/video-frame-interpolation.html">
          <div class="image" style="background-image: url(/assets/img/benchmarks/vfi/main.webp)"></div>
          <div class="title">MSU Video Frame Interpolation Benchmark 2022</div>
          <div class="text">Discover the best algorithm to make high-quality and smooth slow motion videos</div>
        </a>
      </div>
      <div class="tiles-width separator"></div>
      <div class="tiles-width markdown site-structure">
        <span class="title">Site structure</span>
        <ul>
          <li>
            <a href="/benchmarks/">MSU Benchmark Collection</a>
            <ul>
              <li><a href="/benchmarks/deblurring.html">MSU Video Deblurring Benchmark 2022</a></li>
              <li><a href="/benchmarks/video-frame-interpolation.html">MSU Video Frame Interpolation Benchmark 2022</a></li>
              <li><a href="/benchmarks/video-upscalers.html">MSU Video Upscalers Benchmark 2022</a></li>
              <li><a href="/benchmarks/inverse-tone-mapping.html">MSU HDR Video Reconstruction Benchmark 2022</a></li>
              <li><a href="/benchmarks/super-resolution-for-video-compression.html">MSU Super-Resolution for Video Compression Benchmark 2022</a></li>
              <li><a href="/benchmarks/no-reference-video-quality-metrics.html">MSU No-Reference Video Quality Metrics Benchmark 2022</a></li>
              <li><a href="/benchmarks/full-reference-video-quality-metrics.html">MSU Full-Reference Video Quality Metrics Benchmark 2022</a></li>
              <li><a href="/benchmarks/aligners.html">MSU Video Alignment and Retrieval Benchmark</a></li>
              <li><a href="/benchmarks/mobile-video-codec-benchmark.html">MSU Mobile Video Codecs Benchmark 2021</a></li>
              <li><a href="/benchmarks/video-super-resolution.html">MSU Video Super-Resolution Benchmark</a></li>
              <li><a href="/benchmarks/shot-boundary-detection.html">MSU Shot Boundary Detection Benchmark 2020</a></li>
              <li><a href="/benchmarks/deinterlacer.html">MSU Deinterlacer Benchmark</a></li>
              <li><a href="https://videomatting.com/" target="_blank">The VideoMatting Project</a></li>
              <li><a href="https://videocompletion.org/" target="_blank">Video Completion</a></li>
            </ul>
          </li>
          <li>
            <a href="/codecs/">Codecs Comparisons & Optimization</a>
            <ul>
              <li><a href="/codecs/avc/">AVC Codecs Comparison</a></li>
              <li><a href="/codecs/hevc/">HEVC Codecs Comparison</a></li>
              <li><a href="/codecs/image/">Image Codecs Comparison</a></li>
              <li><a href="/codecs/lossless/">Lossless Codecs Comparison</a></li>
              <li><a href="/codecs/optimization/">Codecs Optimization and Tuning</a></li>
              <li><a href="/codecs/reports/">All Codecs Comparison Reports</a></li>
            </ul>
          </li>
          <li>
            <a href="/vqmt/">VQMT</a>
            <ul>
              <li><a href="/vqmt/plugins/">VQMT Plugins</a></li>
            </ul>
          </li>
          <li>
            <a href="/stereo_quality/">Video Quality Measurement Tool 3D</a>
            <ul>
              <li><a href="/stereo_quality/correction/">Stereo Artifacts Correction</a></li>
              <li><a href="/stereo_quality/metrics/">Stereo Quality Metrics</a></li>
              <li><a href="/stereo_quality/reports/">All Stereo Quality Reports</a></li>
            </ul>
          </li>
          <li>
            <a href="/datasets/">MSU Datasets Collection</a>
            <ul>
            </ul>
          </li>
          <li>
            <a href="/metrics/">Metrics Research</a>
            <ul>
            </ul>
          </li>
          <li>
            <a href="/video_filters/">Video Filters</a>
            <ul>
              <li><a href="/video_filters/image/">Image Processing Filters</a></li>
              <li><a href="/video_filters/public/">Free Filters</a></li>
              <li><a href="/video_filters/virtualdub/">VirtualDub Filters</a></li>
            </ul>
          </li>
          <li>
            <a href="/other/">Other Projects</a>
            <ul>
            </ul>
          </li>
        </ul>
      </div>
    </div>
    <div class="footer">
      <div class="footer-column copyright">
        <img alt="MSU Graphics & Multimedia Lab Video Group" class="logo" src="/assets/img/logo.svg">
        <div class="text">
          MSU Graphics & Media Lab Video Group
          <br>
          2019&ndash;2022
        </div>
      </div>
      <div class="footer-column">
        <ul>
          <li><a href="/codecs/">Codecs Comparisons & Optimization</a></li>
          <li><a href="/vqmt/">VQMT</a></li>
          <li><a href="/stereo_quality/">Video Quality Measurement Tool 3D</a></li>
          <li><a href="/datasets/">MSU Datasets Collection</a></li>
          <li><a href="/metrics/">Metrics Research</a></li>
          <li><a href="/video_filters/">Video Filters</a></li>
          <li><a href="/other/">Other Projects</a></li>
        </ul>
      </div>
      <div class="footer-column">
        <ul>
          <li><a href="/about/">About Us</a></li>
          <li><a href="/benchmarks/">Benchmarks</a></li>
          <li><a href="/projects/">Projects</a></li>
          <li><a href="/publications/">Publications</a></li>
          <li><a href="/contacts/">Contacts</a></li>
        </ul>
      </div>
    </div>
  </body>
</html>