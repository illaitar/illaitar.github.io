<div id="comm_comp_div" class="datatable-container">
    <div class="datatable-center">
        <table id="char_table" class="display">
            <thead>
                <tr>
                    <th style="background-color: #3d6f96; color: #ffffff">Name</th>
                    <th style="background-color: #3d6f96; color: #ffffff">Multi-frame</th>
                    <th style="background-color: #3d6f96; color: #ffffff">Integrate temporal information by</th>
                    <th style="background-color: #3d6f96; color: #ffffff">Training dataset</th>
                    <th style="background-color: #3d6f96; color: #ffffff">Framework</th>
                    <th style="background-color: #3d6f96; color: #ffffff">Year</th>
                </tr>
            </thead>
            <tbody>
                <tr class="item">
                    <td><a href="#d3dnet">D3Dnet</a></td>
                    <td style="color: #218838">Yes</td>
                    <td>Deformable convolution</td>
                    <td><a href="https://paperswithcode.com/dataset/vimeo90k-1">Vimeo-90k</a></td>   
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#dbvsr">DBVSR</a></td>
                    <td style="color: #218838">Yes</td>
                    <td>Optical flow</td>
                    <td><a href="https://seungjunnah.github.io/Datasets/reds.html">REDS</a></td>   
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#duf">DUF</a></td>
                    <td style="color: #218838">Yes</td>
                    <td>Deformable convolution</td>
                    <td>Unpublished</td>
                    <td>TensorFlow</td>
                    <td>2018</td>
                </tr>
                <tr class="item">
                    <td><a href="#dynavsr">DynaVSR-V</a></td>
                    <td style="color: #218838">Yes</td>
                    <td></td>
                    <td><a href="https://paperswithcode.com/dataset/vimeo90k-1">Vimeo-90k</a></td>
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#dynavsr">DynaVSR-R</a></td>
                    <td style="color: #218838">Yes</td>
                    <td></td>
                    <td><a href="https://seungjunnah.github.io/Datasets/reds.html">REDS</a></td>
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#espcn">ESPCN</a></td>
                    <td style="color: #d11d12">No</td>
                    <td>—</td>
                    <td><a href="http://www.image-net.org/">ImageNet</a></td>   
                    <td>PyTorch</td>
                    <td>2016</td>
                </tr>
                <tr class="item">
                    <td><a href="#esrgan">ESRGAN</a></td>
                    <td style="color: #d11d12">No</td>
                    <td>—</td>
                    <td></td>   
                    <td>PyTorch</td>
                    <td>2018</td>
                </tr>
                <tr class="item">
                    <td><a href="#iseebetter">iSeeBetter</a></td>
                    <td style="color: #218838">Yes</td>
                    <td>Recurrent architecture + optical flow</td>
                    <td></td>
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#lgfn">LGFN</a></td>
                    <td style="color: #218838">Yes</td>
                    <td>Deformable convolution</td>
                    <td><a href="https://paperswithcode.com/dataset/vimeo90k-1">Vimeo-90k</a></td>
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#rbpn">RBPN</a></td>
                    <td style="color: #218838">Yes</td>
                    <td>Recurrent architecture + optical flow</td>
                    <td><a href="https://paperswithcode.com/dataset/vimeo90k-1">Vimeo-90k</a></td>
                    <td>PyTorch</td>
                    <td>2019</td>
                </tr>
                <tr class="item">
                    <td><a href="#rrn">RRN</a></td>
                    <td style="color: #218838">Yes</td>
                    <td>Recurrent architecture</td>
                    <td><a href="https://paperswithcode.com/dataset/vimeo90k-1">Vimeo-90k</a></td>
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#real-esrgan">Real-ESRGAN</a></td>
                    <td style="color: #d11d12">No</td>
                    <td>—</td>
                    <td></td>   
                    <td>PyTorch</td>
                    <td>2021</td>
                </tr>
                <tr class="item">
                    <td><a href="#real-esrnet">Real-ESRNet</a></td>
                    <td style="color: #d11d12">No</td>
                    <td>—</td>
                    <td></td>   
                    <td>PyTorch</td>
                    <td>2021</td>
                </tr>
                <tr class="item">
                    <td><a href="#realsr">RealSR</a></td>
                    <td style="color: #d11d12">No</td>
                    <td>—</td>
                    <td>DF2K, DPED</td>
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#tga">RSDN</a></td>
                    <td style="color: #218838">Yes</td>
                    <td>Recurrent</td>
                    <td><a href="https://paperswithcode.com/dataset/vimeo90k-1">Vimeo-90k</a></td>   
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#sof-vsr">SOF-VSR</a></td>
                    <td style="color: #218838">Yes</td>
                    <td>Optical flow</td>
                    <td><a href="https://www.cdvl.org">CDVL</a></td>
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#tdan">TDAN</a></td>
                    <td style="color: #218838">Yes</td>
                    <td>Deformable convolution</td>
                    <td><a href="https://paperswithcode.com/dataset/vimeo90k-1">Vimeo-90k</a></td>
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#tga">TGA</a></td>
                    <td style="color: #218838">Yes</td>
                    <td>Temporal Group Attention</td>
                    <td><a href="https://paperswithcode.com/dataset/vimeo90k-1">Vimeo-90k</a></td>   
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#tmnet">TMNet</a></td>
                    <td style="color: #218838">Yes</td>
                    <td>Deformable convolution</td>
                    <td><a href="https://paperswithcode.com/dataset/vimeo90k-1">Vimeo-90k</a></td>   
                    <td>PyTorch</td>
                    <td>2021</td>
                </tr>
            </tbody>
        </table>

        <script>$(document).ready(function () {
            $('#char_table').DataTable({
                lengthChange: false,
                paging: false,
                searching: false
            });
        });</script>
    </div>
</div>

<h2 id="d3dnet">D3Dnet</h2>
<ul>Use a deformable 3D convolution to compensate the motion between frames implicitly.</ul>
<div class="center">
	<div style="width:75%">
		<img src="/assets/img/benchmarks/vsr/d3dnet.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/XinyiYing/D3Dnet">GitHub</a>, <a href="https://doi.org/10.1109/LSP.2020.3013518">paper</a> </ul>

<h2 id="dbvsr">DBVSR</h2>
<ul>Estimate a motion blur for the particular input. Compensate the motion between frames explicitly.</ul>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/vsr/dbvsr.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/cscss/DBVSR">GitHub</a>, <a href="https://arxiv.org/abs/2003.04716">paper</a></ul>

<h2 id="duf">DUF</h2>
<ul>Two models: DUF-16L (16 layers), DUF-28L (28 layers)</ul>
<ul>Use a deformable 3D convolution to compensate the motion between frames implicitly.</ul>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/vsr/duf.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/yhjo09/VSR-DUF">GitHub</a>, <a href="https://doi.org/10.1109/CVPR.2018.00340">paper</a></ul>  

<h2 id="dynavsr">DynaVSR</h2>
<ul>Two models: DynaVSR-R (trained on <a href="https://seungjunnah.github.io/Datasets/reds.html">REDS</a>), DynaVSR-V (trained on <a href="https://paperswithcode.com/dataset/vimeo90k-1">Vimeo-90k</a>)</ul>
<ul>Use meta-learning to estimate a degradation kernel for the particular input.</ul>
<ul>DynaVSR can be applied to any VSR deep-learning model. For our benchmark, we used pretrained weights for model EDVR, which use Deformable convolution to align neighboring frames.</ul>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/vsr/dynavsr.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/esw0116/DynaVSR">GitHub</a>, <a href="http://arxiv.org/abs/2011.04482">paper</a></ul>

<h2 id="espcn">ESPCN</h2>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/vsr/espcn.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/leftthomas/ESPCN">GitHub</a>, <a href="https://arxiv.org/abs/1609.05158">paper</a></ul>

<h2 id="esrgan">ESRGAN</h2>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/vsr/esrgan.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/xinntao/ESRGAN">GitHub</a>, <a href="https://doi.org/10.1007/978-3-030-11021-5_5">paper</a></ul>

<h2 id="iseebetter">iSeeBetter</h2>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/vsr/iseebetter.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/amanchadha/iSeeBetter">GitHub</a>, <a href="https://arxiv.org/abs/2006.11161">paper</a></ul>

<h2 id="lgfn">LGFN</h2>
<ul>Use deformable convolutions with decreased multi-dilation convolution units (DMDCUs) to align frames explicitly. Fuse features from local and global fusion modules.</ul>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/vsr/lgfn.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/BIOINSu/LGFN">GitHub</a>, <a href="https://doi.org/10.1109/ACCESS.2020.3025780">paper</a></ul>

<h2 id="rbpn">RBPN</h2>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/vsr/rbpn.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/alterzero/RBPN-PyTorch">GitHub</a>, <a href="https://arxiv.org/abs/1903.10128">paper</a></ul>

<h2 id="real-esrgan">Real-ESRGAN</h2>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/xinntao/Real-ESRGAN">GitHub</a>, <a href="https://arxiv.org/abs/2107.10833">paper</a></ul>

<h2 id="real-esrnet">Real-ESRNet</h2>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/xinntao/Real-ESRGAN">GitHub</a>, <a href="https://arxiv.org/abs/2107.10833">paper</a></ul>

<h2 id="realsr">RealSR</h2>
<ul>Try to estimate degradation kernel and noise distribution for better visual quality.</ul>
<div class="center">
	<div style="width:75%">
		<img src="/assets/img/benchmarks/vsr/realsr.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/nihui/realsr-ncnn-vulkan">GitHub</a>, <a href="https://github.com/jixiaozhong/RealSR">GitHub</a>, <a href="https://doi.org/10.1109/CVPRW50498.2020.00241">paper</a></ul>

<h2 id="rrn">RRN</h2>
<ul>Two models: RRN-5L (five residual blocks), RRN-10L (ten residual blocks)</ul>
<ul>Use recurrent strategy with sets of residual blocks to store information from previous frames.</ul>
<div class="center">
	<div style="width:75%">
		<img src="/assets/img/benchmarks/vsr/rrn.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/junpan19/RRN">GitHub</a>, <a href="https://arxiv.org/abs/2008.05765v2">paper</a></ul>

<h2 id="rsdn">RSDN</h2>
<ul>It divides the input into structure and detail components which are fed to a recurrent unit composed of several proposed two-stream structure-detail blocks.</ul>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/vsr/rsdn.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/junpan19/RSDN">GitHub</a>, <a href="https://arxiv.org/abs/2008.00455">paper</a></ul>

<h2 id="sof-vsr">SOF-VSR</h2>
<ul>Two models: SOF-VSR-BD (trained on gauss degradation type), SOF-VSR-BI (trained on bicubic degradation type)</ul>
<ul>Compensate motion by high-resolution optical flow, estimated from the low-resolution one in a coarse-to-fine manner.</ul>
<div class="center">
	<div style="width:75%">
		<img src="/assets/img/benchmarks/vsr/sof-vsr.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/LongguangWang/SOF-VSR">GitHub</a>, <a href="https://doi.org/10.1109/TIP.2020.2967596">paper</a></ul>

<h2 id="tdan">TDAN</h2>
<ul>Use a deformable 3D convolution to compensate the motion between frames implicitly.</ul>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/vsr/tdan.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/YapengTian/TDAN-VSR-CVPR-2020">GitHub</a>, <a href="https://arxiv.org/abs/1812.02898">paper</a></ul>

<h2 id="tga">TGA</h2>
<ul>The input sequence is reorganized into several groups of subsequences with different frame rates. The grouping allows to extract spatio-temporal information in a hierarchical manner, which is followed by an intra-group fusion module and inter-group fusion module.</ul>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/vsr/tga.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/junpan19/VSR_TGA">GitHub</a>, <a href="https://doi.org/10.1109/cvpr42600.2020.00803">paper</a></ul>

<h2 id="tmnet">TMNet</h2>
<ul>Temporal Modulation Network was trained for Space-Time Video Super Resolution. The temporal information is integrated by deformable convolution with the multi-frame input.</ul>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/vsr/tmnet.jpg">
	</div>
</div>
<ul>Added to the benchmark by the author, Gang Xu.</ul>
<ul>Links: <a href="https://github.com/CS-GangXu/TMNet">GitHub</a>, <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Temporal_Modulation_Network_for_Controllable_Space-Time_Video_Super-Resolution_CVPR_2021_paper.pdf">paper</a></ul>