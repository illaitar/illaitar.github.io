<!DOCTYPE html>
<html lang="en">
  <head prefix="og: https://ogp.me/ns#">
    <link href="/assets/favicon/favicon.ico" rel="shortcut icon">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="UTF-8">
    <title>MSU Video Deblurring Benchmark Methodology</title>
    <meta property="og:title" content="MSU Video Deblurring Benchmark Methodology">
    <meta property="og:image" content="http://localhost:4000/assets/img/logo.svg">
    <meta name="description" content="The evaluation methodology of MSU Video Deblurring Benchmark">
    <meta property="og:description" content="The evaluation methodology of MSU Video Deblurring Benchmark">
    <meta property="og:url" content="
http://localhost:4000/benchmarks/deblurring-methodology.html
">
    <meta property="og:type" content="website">
    <link href="/assets/css/common.css" rel="stylesheet" type="text/css">
    <script src="/assets/js/interface.js"></script>
    <link rel="stylesheet" type="text/css" href="/assets/css/nav_arrow.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  </head>
  <body>
    <ul class="navbar">
      <li class="navbar-header">
        <a href="/" class="header">
          <img alt="Main page" class="logo" src="/assets/img/logo.svg">
          <h1>Video processing, compression<br>
            and quality research group
            <div>Based in MSU Graphics & Media Laboratory</div>
          </h1>
        </a>
        <a class="menu-toggle-button">
          <div class="icon"></div>
        </a>
      </li>
      <li>
        <a href="/benchmarks/">Benchmarks <img class="dropdown-icon" src="/assets/icons/dropdown.svg"></a>
        <ul class="dropmenu">
          <li><a href="/benchmarks/deblurring.html">MSU Video Deblurring Benchmark 2022</a></li>
          <li><a href="/benchmarks/video-frame-interpolation.html">MSU Video Frame Interpolation Benchmark 2022</a></li>
          <li><a href="/benchmarks/video-upscalers.html">MSU Video Upscalers Benchmark 2022</a></li>
          <li><a href="/benchmarks/inverse-tone-mapping.html">MSU HDR Video Reconstruction Benchmark 2022</a></li>
          <li><a href="/benchmarks/super-resolution-for-video-compression.html">MSU Super-Resolution for Video Compression Benchmark 2022</a></li>
          <li><a href="/benchmarks/no-reference-video-quality-metrics.html">MSU No-Reference Video Quality Metrics Benchmark 2022</a></li>
          <li><a href="/benchmarks/full-reference-video-quality-metrics.html">MSU Full-Reference Video Quality Metrics Benchmark 2022</a></li>
          <li><a href="/benchmarks/aligners.html">MSU Video Alignment and Retrieval Benchmark</a></li>
          <li><a href="/benchmarks/mobile-video-codec-benchmark.html">MSU Mobile Video Codecs Benchmark 2021</a></li>
          <li><a href="/benchmarks/video-super-resolution.html">MSU Video Super-Resolution Benchmark</a></li>
          <li><a href="/benchmarks/shot-boundary-detection.html">MSU Shot Boundary Detection Benchmark 2020</a></li>
          <li><a href="/benchmarks/deinterlacer.html">MSU Deinterlacer Benchmark</a></li>
          <li><a href="https://videomatting.com/" target="_blank">The VideoMatting Project</a></li>
          <li><a href="https://videocompletion.org/" target="_blank">Video Completion</a></li>
        </ul>
      </li>
      <li>
        <a href="/projects/">Projects <img class="dropdown-icon" src="/assets/icons/dropdown.svg"></a>
        <ul class="dropmenu">
          <li><a href="/codecs/">Codecs Comparisons & Optimization</a></li>
          <li><a href="/vqmt/">VQMT</a></li>
          <li><a href="/stereo_quality/">Video Quality Measurement Tool 3D</a></li>
          <li><a href="/datasets/">MSU Datasets Collection</a></li>
          <li><a href="/metrics/">Metrics Research</a></li>
          <li><a href="/video_filters/">Video Filters</a></li>
          <li><a href="/other/">Other Projects</a></li>
        </ul>
      </li>
      <li><a href="/publications/">Publications</a></li>
      <li><a href="/about/">About Us</a></li>
      <li><a href="/contacts/">Contacts</a></li>
    </ul>
    <div class="content">
      <link href="/assets/css/post.css" rel="stylesheet" type="text/css">
      <div class="tiles-width nav-current">
        <a href="/index.html">Main page</a> &mdash;
        <a href="/benchmarks/">MSU Benchmark Collection</a>
      </div>
      <div class="tiles-width markdown article">
        <link rel="stylesheet" href="/assets/css/benchmarks/style.css" />
        <script src="https://code.highcharts.com/highcharts.js"></script>
        <script src="https://code.highcharts.com/modules/exporting.js"></script>
        <script src="https://code.highcharts.com/modules/export-data.js"></script>
        <script src="https://code.highcharts.com/modules/accessibility.js"></script>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
        <script src="https://code.highcharts.com/highcharts-more.js"></script>
        <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.10.22/css/jquery.dataTables.css" />
        <script type="text/javascript" charset="utf8" src="https://cdn.datatables.net/1.10.22/js/jquery.dataTables.js"></script>
        <h1 id="evaluation-methodology-of-msu-video-deblurring-2022">Evaluation methodology of MSU Video Deblurring 2022</h1>
        <div id="buttons"></div>
        <script>
          __set_menu_buttons([
          ['Home', '/benchmarks/deblurring.html'],
          ['Participants','/benchmarks/deblurring-participants.html'],
          ['Dataset','/benchmarks/deblurring-dataset.html'],
          ['Evaluation methodology', '/benchmarks/deblurring-methodology.html'],
          ['How to participate', '/benchmarks/deblurring.html#participate'],
              ['Contact us', '/benchmarks/deblurring.html#contacts']
          ], 'Evaluation methodology')
        </script>
        <div class="current_content">
          <style>
            table, th, td {
                border: 2px solid black;
                border-collapse: collapse;
            }
            
            .width {
                width: 85%;
            }
            
            .width2 {
                width: 80%;
            }
            
            th, td {
                padding: 10px;
            }
            
            .column {
              float: left;
              width: 45%;
              padding: 5px;
            }
            
            .row::after {
              content: "";
              clear: both;
              display: table;
            }
            
            tt {
                font-family: "Lucida Console", "Menlo", "Monaco", "Courier",
                             monospace;
              }
          </style>
          <!--
<p>You can read the Methodology below or download the presentation in pdf format 
    <a href="https://drive.google.com/file/d/1Fw5RkfPCvjpYxoCV-2n-Qr7oYLdqEnAO/view?usp=sharing">here</a>.<br> You also can see it in 
    Google Slides <a href="https://docs.google.com/presentation/d/1od9wMfacW-2p3enQHUSjKV5R3kcbnwdk3YSf-cqi_d4/edit?usp=sharing">here</a>.</p>
-->
          <h2 id="problem_definition">Problem definition</h2>
          <p>
            Deblurring is the process of removing blurring artifacts from images. Video deblurring recovers a sharp sequence from a blurred one. Current SOTA aproaches use deep learning algorythms for this task. Our benchmark will rank these algorithms and determine which is the best by means of restoration quality.
          </p>
          <p>
            To propose quality comparison we analyze different deblurring datasets. Many of them use gaussian blur to emulate defocus blur. For example, the most popolar datasets on paperswithcode, GoPro and REDS, both incorporate synthetic method of blur creation. RealBlur was the first dataset to suggest using a beam splitter to shoot with real distortion.  RealBlur has many scenes with nonprecise matched blurred and ground truth frames because of parallax effect. Nevertheless, they show significant gains achieved by learning from real distortion.
          </p>
          <h2 id="dataset">Dataset Proposal</h2>
          <p>
            We propose a new real motion blur dataset. Using a beam-splitter rig and two GoPro Hero 9 cameras, we filmed different scenes. We set the stereo base between two cameras to zero and obtain the same scene from two cameras with different shutter speed settings. The camera with higher shutter speed (Camera [2]) captures ground truth frames, while the camera with lower shutter speed (Camera [3]) captures motion blur.
          </p>
          <div class="center">
            <div>
              <img style="width:100%" src="/assets/img/benchmarks/deblurring/scheme.png" />
            </div>
          </div>
          <h3 id="dataset">Beam-splitter</h3>
          <p>
            We construct a box to protect our cameras and beam splitter from external light.  To be able to more precisely manually align cameras and to raise the camera above the beam-splitter glass holder, we construct some stands and screw cameras to them. Then we connect the power and data cables to the cameras. Also, wThe walls of the box were covered with black matte sheets of paper for better light absorption and to increase the reflective properties of the beam-splitter at the time of shooting. The beam splitter itself was vertically aligned at 90 degrees angle.
          </p>
          <div class="center">
            <div>
              <img style="width:80%" src="/assets/img/benchmarks/deblurring/splitter.png" />
            </div>
          </div>
          <h3 id="alignment">Alignment</h3>
          <p>
            We disable optical stabilization and manually set all the available settings, such as ISO, focus distance, and color temperature. We film our sequences in 4K 60 FPS, and then crop regions of interest.
          </p>
          <p>
            To achieve precise ground truth data it is crucial to film scenes without parallax present. The parallax is a difference in apparent position when an object is viewed along the two lines of sight. During post-processing, we can precisely correct affine mismatches between cameras, namely scale, rotation angle, and transition, but we cannot correct parallax without using complex optical flow algorithms that can affect our ground truth data quality.
          </p>
          <p>
            For coarse alignment, we firstly visually aligned the lenses to obtain a visual zero stereo base. Then we point two cameras at one scene, using a photo from one camera and a video stream from another. After making sure that the parallax is not present we film all scenes in one go.
          </p>
          <h3 id="scenes">Scene selection</h3>
          <p>
            We use simple scenes to focus on a complex analysis of motion blur. We placed objects on a table with a white background 1.5 meters from the camera. We have aquired 23 sequences with different types of motion and objects using two GoPro Hero 9 cameras.
          </p>
          <h3 id="processing">Post-processing</h3>
          <p>
            For post-processing, we use a simple pipeline to minimize ground truth data tampering. Given the two videos from cameras, we first horizontally flip the blurred view, then we match the blurred view to the ground truth view using a homography transformation. For the estimation of transformation parameters, we utilize SIFT features, FLANN matcher, and the RANSAC algorithm. To correct beam splitter color distortions we use VQMT3D[link] color correction algorithm.
          </p>
          <!-- 
<div class="center" style="padding-bottom: 0px; margin-bottom: 0px;">
    <video autoplay loop muted playsinline width="100%">
        <source src="/assets/img/benchmarks/sr-codecs/dataset_preview_.mp4" type='video/mp4'>
        Sorry, your browser does not support this video.
    </video>
</div>

<div class="center">
    <p><i>Figure 1. Segments from dataset</i></p>
</div> -->
          <!--
<div class="center">
    <div style="width:75%">
        <video autoplay loop muted playsinline>
            <source src="/assets/img/benchmarks/sr-codecs/dataset_preview.mp4" type='video/mp4'>
            <source src="/assets/img/benchmarks/sr-codecs/dataset_preview.vp9.webm" type='video/webm'>
            <source src="/assets/img/benchmarks/sr-codecs/dataset_preview.av1.mp4" type='video/mp4'>
        </video>
        <p><i>Figure 1. Segments from dataset</i></p>
    </div>
</div>

<div id="dataset_table" class="center">
    <table class="width2" style="background-color: #ffffff;">
        <thead>
            <tr style="font-size: medium">
                <td style="text-align:center">
                    <p><b>Preview</b></p>
                </td>
                <td style="text-align:center">
                    <p><b>Description</b></p>
                </td>
                <td style="text-align:center">
                    <p><b>Format</b></p>
                </td>
                <td style="text-align:center">
                    <p><b>Frames</b></p>
                </td>
                <td style="text-align:center">
                    <p><b>FPS</b></p>
                </td>
                <td style="text-align:center">
                    <p><b>Bitrate</b></p>
                </td>
            </tr>
        </thead>
        <tbody>
            <tr style="font-size: medium">
                <td style="text-align: center;"> 
                    <div>
                        <img width="100%" src="/assets/img/benchmarks/sr-codecs/animation_clip-small.png">
                    </div>
                </td>
                <td style="text-align:center">
                    <p><b>Animation clip</b></p>
                    <p>2D animation advertising clip<br>drawn in bright colors.</p>
                </td>
                <td style="text-align:center">
                    <p><br>FullHD</p>
                </td>
                <td style="text-align:center">
                    <p><br>100</p>
                </td>
                <td style="text-align:center">
                    <p><br>30 fps</p>
                </td>
                <td style="text-align:center">
                    <p><br>104.58 Mbps</p>
                </td>
            </tr>
            <tr style="font-size: medium">
                <td style="text-align: center;"> 
                    <div>
                        <img width="100%" src="/assets/img/benchmarks/sr-codecs/glass-small.png">
                    </div>
                </td>
                <td style="text-align:center">
                    <p><b>Glass</b></p>
                    <p>A man working with an oven<br>in the workshop.</p>
                </td>
                <td style="text-align:center">
                    <p><br>FullHD</p>
                </td>
                <td style="text-align:center">
                    <p><br>105</p>
                </td>
                <td style="text-align:center">
                    <p><br>30 fps</p>
                </td>
                <td style="text-align:center">
                    <p><br>108.14 Mbps</p>
                </td>
            </tr>
            <tr style="font-size: medium; ">
                <td style="text-align: center;">
                    <div>
                        <img style="align: center;" width="100%" src="/assets/img/benchmarks/sr-codecs/lift-small.png">
                    </div>
                </td>
                <td style="text-align:center">
                    <p><b>Lift</b></p>
                    <p>Animation clip,<br>slow camera zoom.</p>
                </td>
                <td style="text-align:center">
                    <p><br>FullHD</p>
                </td>
                <td style="text-align:center">
                    <p><br>120</p>
                </td>
                <td style="text-align:center">
                    <p><br>30 fps</p>
                </td>
                <td style="text-align:center">
                    <p><br>104.56 Mbps</p>
                </td>
            </tr>
            <tr style="font-size: medium">
                <td style="text-align: center;"> 
                    <div>
                        <img width="100%" src="/assets/img/benchmarks/sr-codecs/professor-small.png">
                    </div>
                </td>
                <td style="text-align:center">
                    <p><b>Professor</b></p>
                    <p>A scene with professor from<br>international affairs school.</p>
                </td>
                <td style="text-align:center">
                    <p><br>FullHD</p>
                </td>
                <td style="text-align:center">
                    <p><br>105</p>
                </td>
                <td style="text-align:center">
                    <p><br>24 fps</p>
                </td>
                <td style="text-align:center">
                    <p><br>165.41 Mbps </p>
                </td>
            </tr>
            <tr style="font-size: medium;">
                <td style="text-align: center;">
                    <div>
                        <img width="100%" src="/assets/img/benchmarks/sr-codecs/skiing-small.png">
                    </div>
                </td>
                <td style="text-align:center">
                    <p><b>Skiing learning</b></p>
                    <p>People are being trained to ski<br>in slow motion.</p>
                </td>
                <td style="text-align:center">
                    <p><br>FullHD</p>
                </td>
                <td style="text-align:center">
                    <p><br>179</p>
                </td>
                <td style="text-align:center">
                    <p><br>24 fps</p>
                </td>
                <td style="text-align:center">
                    <p><br>107.59 Mbps</p>
                </td>
            </tr>
            <tr style="font-size: medium; ">
                <td style="text-align: center;">
                    <div>
                        <img style="align: center;" width="100%" src="/assets/img/benchmarks/sr-codecs/street_show-small.png">
                    </div>
                </td>
                <td style="text-align:center">
                    <p><b>Street show</b></p>
                    <p>Two men sing, dance and perform<br>some acrobatics on a street.</p>
                </td>
                <td style="text-align:center">
                    <p><br>FullHD</p>
                </td>
                <td style="text-align:center">
                    <p><br>200</p>
                </td>
                <td style="text-align:center">
                    <p><br>24 fps</p>
                </td>
                <td style="text-align:center">
                    <p><br>108.40 Mbps</p>
                </td>
            </tr>
            <tr style="font-size: medium">
                <td style="text-align: center;"> 
                    <div>
                        <img width="100%" src="/assets/img/benchmarks/sr-codecs/wedding-small.png">
                    </div>
                </td>
                <td style="text-align:center">
                    <p><b>Wedding</b></p>
                    <p>A couple walks slowly<br>at the ceremony.</p>
                </td>
                <td style="text-align:center">
                    <p><br>FullHD</p>
                </td>
                <td style="text-align:center">
                    <p><br>125</p>
                </td>
                <td style="text-align:center">
                    <p><br>24 fps</p>
                </td>
                <td style="text-align:center">
                    <p><br>123.96 Mbps</p>
                </td>
            </tr>
        </tbody>
    </table>
</div>
<p class="center"><i>Table 2: Dataset characteristics</i></p>
-->
          <h2 id="metrics">Metrics</h2>
          <h4 id="psnr">PSNR</h4>
          <p>
            PSNR is a commonly used metric for reconstruction quality for images and video. 
            In our benchmark, we calculate PSNR on the Y component in YUV colorspace. 
          </p>
          <p>
            Since some Super-Resolution models can generate images with a global shift relative to GT, we calculate shifted PSNR. 
            We check each shift in the range [-3, 3] (including subpixel shifts) for both axes and select the highest PSNR value among these shifts. 
            We noticed that SRs’ results on the same video decoded with different bitrates usually have the same global shift. 
            Thus we calculate the best shift only once for each video.
          </p>
          <p>For metric calculation, we use MSU VQMT<sup><a href="#references">[2]</a></sup>.</p>
          <h4 id="ssim">SSIM</h4>
          <p>
            SSIM is a metric based on structural similarity. 
            <sup><a href="#references">[13]</a></sup>.
          </p>
          <p>
            These results also rely on the shift of frames. 
            We take optimal subpixel shift for PSNR and apply in to input frames 
            before calculating MS-SSIM.
          </p>
          <p>For metric calculation, we use MSU VQMT<sup><a href="#references">[2]</a></sup>.</p>
          <h4 id="vmaf">VMAF</h4>
          <p>
            VMAF is a perceptual video quality assessment algorithm developed by Netflix. 
            In our benchmark, we calculate VMAF on the Y component in YUV colorspace. 
          </p>
          <p>
            For metric calculation, we use MSU VQMT<sup><a href="#references">[2]</a></sup>. 
            For VMAF we use <code>-set "disable_clip=True"</code> option of MSU VQMT.
          </p>
          <!--
<p>
    Shifted VMAF and VMAF NEG give less than 1% gain relative to unshifted versions, 
    that’s why we use unshifted versions in our benchmark.
    In Figure 2a and 2b you can see the gain that each model get by using shifted VMAF and VMAF NEG relative to unshifted versions.
</p>

<div class="center">
    <div class="row">
        <div class="column">
            <img style="width:100%" src="/assets/img/benchmarks/sr-codecs/vmaf_gain.png">
            <p><i>Figure 2a. Shifted VMAF gain of each model</i></p>
        </div>
        <div class="column">
            <img style="width:100%" src="/assets/img/benchmarks/sr-codecs/vmaf_neg_gain.png">
            <p><i>Figure 2b. Shifted VMAF NEG gain of each model</i></p>
        </div>
    </div>
</div>
-->
          <h4 id="lpips">LPIPS</h4>
          <p>
            LPIPS (Learned Perceptual Image Patch Similarity) evaluates the distance between image patches. 
            Higher means further/more different. Lower means more similar. In our benchmark, we subtract LPIPS value from 1. 
            Thus, more similar images have higher metric values. 
          </p>
          <p>
            To calculate LPIPS we use Perceptual Similarity Metric implementation<sup><a href="#references">[3]</a></sup> proposed in 
            The Unreasonable Effectiveness of Deep Features as a Perceptual Metric<sup><a href="#references">[4]</a></sup>.
          </p>
          <!--
<p>
    We have also noticed, that shifted LPIPS give less than 1% gain relative to the unshifted version,
    as you can see in Figure 3. That's why we calculate LPIPS without shift compensation in our benchmark.
</p>


<div class="center">
    <div>
        <img style="width:60%" src="/assets/img/benchmarks/sr-codecs/lpips_gain.png">
        <p><i>Figure 3. Shifted LPIPS gain of each model</i></p>
    </div>
</div>
-->
          <h4 id="erqa">ERQA</h4>
          <p>
            ERQAv2.0 (Edge Restoration Quality Assessment, version 2.0) estimates how well 
            a model has restored edges of the high-resolution frame. 
            This metric was developed for MSU Video Super-Resolution Benchmark 2021<sup><a href="#references">[5]</a></sup>.
          </p>
          <p>
            Firstly, we find edges in both output and GT frames. 
            To do it we use OpenCV implementation<sup><a href="#references">[6]</a></sup> of the Canny algorithm<sup><a href="#references">[7]</a></sup>. 
            A threshold for the initial finding of strong edges is set to 200 and a threshold 
            for edge linking is set to 100. Then we compare these edges by using an F1-score. 
            To compensate for the one-pixel shift, edges that are no more than one pixel away 
            from the GT's are considered true-positive. 
          </p>
          <p>
            More information about this metric can be found at the 
            Evaluation Methodology of MSU Video Super-Resolution Benchmark<sup><a href="#references">[9]</a></sup>.
          </p>
          <div class="center">
            <div>
              <img style="width:100%" src="/assets/img/benchmarks/sr-codecs/erqa_visualisation.png" />
              <p><i>Figure 2. ERQAv2.0 visualization.<br />
                  White pixels are True Positive, red pixels are False Positive, blue pixels are False Negative</i></p>
            </div>
          </div>
          <!-- <h3 id="subjective_comparison">Subjective comparison</h3>

<p>
    We have conducted 5 subjective comparisons for each codec separately. We took videos compressed with 3 different bitrates 
    (approximately 600, 1000, and 2000 kbps) and cut one crop of size 480×270 from each video. The crops were chosen based on
    saliency maps generated by the method <sup><a href="#references">[14]</a></sup> proposed in 
    Contextual encoder-decoder network for visual saliency prediction<sup><a href="#references">[15]</a></sup>. 
    We selected 10 visually different SR models for each codec. Each one of 5397 participants has seen
    25 video pairs and had to choose which one of them looks more realistic (option “indistinguishable” is also available). 
    There were 3 verification questions to protect against random answers and bots. You can see the current number of valid answers in <a href="#benchmark_statistics">Table 1</a>.
    We used these valid answers to predict the ranking using the
    Bradley-Terry model. 
</p>
 -->
          <!--<div class="center">
    <div>
        <img style="width:80%" src="/assets/img/benchmarks/sr-codecs/subjective_crop.gif">
        <p><i>Figure 9. Crops used for subjective comparison</i></p>
    </div>
</div>
-->
          <!-- <div class="center" style="padding-bottom: 0px; margin-bottom: 0px;">
    <video autoplay loop muted playsinline width="100%">
        <source src="/assets/img/benchmarks/sr-codecs/crops.mp4" type='video/mp4'>
        Sorry, your browser does not support this video.
    </video>
</div>
<div class="center">
    <p><i>Figure 6. Crops used for subjective comparison.</i></p>
</div>
 -->
          <!--
<h4 id="extrapolation">Subjective BSQ-rate calculation</h4>

<p>
    To calculate subjective BSQ-rate we extrapolated subjective results using the 
    most similar objective metric. To do this we take the subjective results on 3 
    bitrates used for subjective comparison, find the objective metric that has the 
    highest correlation with the subjective one on the same bitrates, and extrapolate 
    subjective metric using this objective metric as a reference (see Figures 10a and 10b).
</p>


<div class="center">
    <div class="row">
        <div class="column">
            <img style="width:100%" src="/assets/img/benchmarks/sr-codecs/extrapolation.png">
            <p><i>Figure 10a. Subjective metric extrapolation</i></p>
        </div>
        <div class="column">
            <img style="width:100%" src="/assets/img/benchmarks/sr-codecs/objective_metric.png">
            <p><i>Figure 10b. The most similar objective metric</i></p>
        </div>
    </div>
</div>

-->
          <!-- <h3 id="computational_complexity">Computational complexity</h3>

<p>
    We run each model on NVIDIA Titan RTX and calculated runtime on the same test sequence:
</p>

<ul>
    <li>100 frames</li>
    <li>Input resolution — 480×270</li>
    <li>
        Test case:
        <ul>
            <li>video — dancing</li>
            <li>codec — x264</li>
            <li>bitrate — 836 kbps</li>
        </ul>
    </li>
</ul>

<p>
    We calculate frames per second (FPS) as the number of sequence frames divided by the execution time of a total model runtime.
</p> -->
          <h3 id="references">References</h3>
          <ol>
            <li><a href="http://compression.ru/video/quality_measure/video_measurement_tool.html">http://compression.ru/video/quality_measure/video_measurement_tool.html</a></li>
            <li><a href="https://github.com/richzhang/PerceptualSimilarity">https://github.com/richzhang/PerceptualSimilarity</a></li>
            <li>R. Zhang, P. Isola, A. A. Efros, E. Shechtman, O. Wang, "The unreasonable effectiveness of deep features as a perceptual metric," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2020, pp.586-595.</li>
            <li><a href="https://videoprocessing.ai/benchmarks/video-super-resolution.html">https://videoprocessing.ai/benchmarks/video-super-resolution.html</a></li>
            <li><a href="https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga04723e007ed888ddf11d9ba04e2232de">https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga04723e007ed888ddf11d9ba04e2232de</a></li>
            <li><a href="https://en.wikipedia.org/wiki/Canny_edge_detector">https://en.wikipedia.org/wiki/Canny_edge_detector</a></li>
            <li><a href="https://videoprocessing.ai/benchmarks/video-super-resolution-methodology.html">https://videoprocessing.ai/benchmarks/video-super-resolution-methodology.html</a></li>
            <li><a href="http://app.subjectify.us/">http://app.subjectify.us/</a></li>
            <li>A. Antsiferova, A. Yakovenko, N. Safonov, D. Kulikov, A. Gushin, D.Vatolin, "Objective video quality metrics application to video codecs comparisons: choosing the best for subjective quality estimation," in arXiv preprint arXiv:2107.10220, 2021</li>
            <li><a href="https://github.com/alexanderkroner/saliency">https://github.com/alexanderkroner/saliency</a></li>
            <li>A. Kroner, M. Senden, K. Driessens, and R. Goebel, "Contextual encoder-decoder network for visual saliency prediction," in Neural Networks, 129, pp. 261-270, 2020.</li>
          </ol>
        </div>
      </div>
      <div class="tiles-width meta">
        <div class="share">
          <link href="/assets/css/sharing-buttons.css" rel="stylesheet" type="text/css">
          <!-- Sharingbutton Twitter -->
          <a class="resp-sharing-button__link"
   href="https://twitter.com/intent/tweet/?text=MSU Video Deblurring Benchmark Methodology&amp;url=
http://localhost:4000/benchmarks/deblurring-methodology.html
"
   target="_blank" rel="noopener" aria-label="">
            <div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small">
              <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5 0-4.55 2.04-4.55 4.54 0 .36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3 0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35 0 12.92-6.92 12.92-12.93 0-.2 0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"/>
              </svg>
            </div>
          </a>
          <!-- Sharingbutton Telegram -->
          <a aria-label=""
   class="resp-sharing-button__link"
   href="https://telegram.me/share/url?text=MSU Video Deblurring Benchmark Methodology&amp;url=
http://localhost:4000/benchmarks/deblurring-methodology.html
" rel="noopener" target="_blank">
            <div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--small">
              <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M.707 8.475C.275 8.64 0 9.508 0 9.508s.284.867.718 1.03l5.09 1.897 1.986 6.38a1.102 1.102 0 0 0 1.75.527l2.96-2.41a.405.405 0 0 1 .494-.013l5.34 3.87a1.1 1.1 0 0 0 1.046.135 1.1 1.1 0 0 0 .682-.803l3.91-18.795A1.102 1.102 0 0 0 22.5.075L.706 8.475z"/>
              </svg>
            </div>
          </a>
          <!-- Sharingbutton Facebook -->
          <a aria-label=""
   class="resp-sharing-button__link"
   href="https://facebook.com/sharer/sharer.php?u=
http://localhost:4000/benchmarks/deblurring-methodology.html
" rel="noopener"
   target="_blank">
            <div class="resp-sharing-button resp-sharing-button--facebook resp-sharing-button--small">
              <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M18.77 7.46H14.5v-1.9c0-.9.6-1.1 1-1.1h3V.5h-4.33C10.24.5 9.5 3.44 9.5 5.32v2.15h-3v4h3v12h5v-12h3.85l.42-4z"/>
              </svg>
            </div>
          </a>
          <!-- Sharingbutton E-Mail -->
          <a class="resp-sharing-button__link"
   href="mailto:?subject=MSU Video Deblurring Benchmark Methodology&amp;body=
http://localhost:4000/benchmarks/deblurring-methodology.html
" target="_self" rel="noopener"
   aria-label="">
            <div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small">
              <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M22 4H2C.9 4 0 4.9 0 6v12c0 1.1.9 2 2 2h20c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zM7.25 14.43l-3.5 2c-.08.05-.17.07-.25.07-.17 0-.34-.1-.43-.25-.14-.24-.06-.55.18-.68l3.5-2c.24-.14.55-.06.68.18.14.24.06.55-.18.68zm4.75.07c-.1 0-.2-.03-.27-.08l-8.5-5.5c-.23-.15-.3-.46-.15-.7.15-.22.46-.3.7-.14L12 13.4l8.23-5.32c.23-.15.54-.08.7.15.14.23.07.54-.16.7l-8.5 5.5c-.08.04-.17.07-.27.07zm8.93 1.75c-.1.16-.26.25-.43.25-.08 0-.17-.02-.25-.07l-3.5-2c-.24-.13-.32-.44-.18-.68s.44-.32.68-.18l3.5 2c.24.13.32.44.18.68z"/>
              </svg>
            </div>
          </a>
          <!-- Sharingbutton LinkedIn -->
          <a class="resp-sharing-button__link"
   href="https://www.linkedin.com/shareArticle?mini=true&amp;url=
http://localhost:4000/benchmarks/deblurring-methodology.html
&amp;title=MSU Video Deblurring Benchmark Methodology&amp;summary=MSU Video Deblurring Benchmark Methodology&amp;source=
http://localhost:4000/benchmarks/deblurring-methodology.html
"
   target="_blank" rel="noopener" aria-label="">
            <div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small">
              <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M6.5 21.5h-5v-13h5v13zM4 6.5C2.5 6.5 1.5 5.3 1.5 4s1-2.4 2.5-2.4c1.6 0 2.5 1 2.6 2.5 0 1.4-1 2.5-2.6 2.5zm11.5 6c-1 0-2 1-2 2v7h-5v-13h5V10s1.6-1.5 4-1.5c3 0 5 2.2 5 6.3v6.7h-5v-7c0-1-1-2-2-2z"/>
              </svg>
            </div>
          </a>
        </div>
        <span class="date">24 Oct 2022</span>
      </div>
      <div class="tiles-width separator"></div>
      <div class="tiles-width see-also-title">
        See Also
      </div>
      <div class="tiles wide">
        <a class="tile" href="/benchmarks/deblurring.html">
          <div class="image" style="background-image: url(/assets/img/benchmarks/deblurring/preview.png)"></div>
          <div class="title">MSU Video Deblurring Benchmark 2022</div>
          <div class="text">Learn about the best video deblurring methods and choose the best model</div>
        </a>
        <a class="tile" href="/benchmarks/video-frame-interpolation.html">
          <div class="image" style="background-image: url(/assets/img/benchmarks/vfi/main.webp)"></div>
          <div class="title">MSU Video Frame Interpolation Benchmark 2022</div>
          <div class="text">Discover the best algorithm to make high-quality and smooth slow motion videos</div>
        </a>
      </div>
      <div class="tiles-width separator"></div>
      <div class="tiles-width markdown site-structure">
        <span class="title">Site structure</span>
        <ul>
          <li>
            <a href="/benchmarks/">MSU Benchmark Collection</a>
            <ul>
              <li><a href="/benchmarks/deblurring.html">MSU Video Deblurring Benchmark 2022</a></li>
              <li><a href="/benchmarks/video-frame-interpolation.html">MSU Video Frame Interpolation Benchmark 2022</a></li>
              <li><a href="/benchmarks/video-upscalers.html">MSU Video Upscalers Benchmark 2022</a></li>
              <li><a href="/benchmarks/inverse-tone-mapping.html">MSU HDR Video Reconstruction Benchmark 2022</a></li>
              <li><a href="/benchmarks/super-resolution-for-video-compression.html">MSU Super-Resolution for Video Compression Benchmark 2022</a></li>
              <li><a href="/benchmarks/no-reference-video-quality-metrics.html">MSU No-Reference Video Quality Metrics Benchmark 2022</a></li>
              <li><a href="/benchmarks/full-reference-video-quality-metrics.html">MSU Full-Reference Video Quality Metrics Benchmark 2022</a></li>
              <li><a href="/benchmarks/aligners.html">MSU Video Alignment and Retrieval Benchmark</a></li>
              <li><a href="/benchmarks/mobile-video-codec-benchmark.html">MSU Mobile Video Codecs Benchmark 2021</a></li>
              <li><a href="/benchmarks/video-super-resolution.html">MSU Video Super-Resolution Benchmark</a></li>
              <li><a href="/benchmarks/shot-boundary-detection.html">MSU Shot Boundary Detection Benchmark 2020</a></li>
              <li><a href="/benchmarks/deinterlacer.html">MSU Deinterlacer Benchmark</a></li>
              <li><a href="https://videomatting.com/" target="_blank">The VideoMatting Project</a></li>
              <li><a href="https://videocompletion.org/" target="_blank">Video Completion</a></li>
            </ul>
          </li>
          <li>
            <a href="/codecs/">Codecs Comparisons & Optimization</a>
            <ul>
              <li><a href="/codecs/avc/">AVC Codecs Comparison</a></li>
              <li><a href="/codecs/hevc/">HEVC Codecs Comparison</a></li>
              <li><a href="/codecs/image/">Image Codecs Comparison</a></li>
              <li><a href="/codecs/lossless/">Lossless Codecs Comparison</a></li>
              <li><a href="/codecs/optimization/">Codecs Optimization and Tuning</a></li>
              <li><a href="/codecs/reports/">All Codecs Comparison Reports</a></li>
            </ul>
          </li>
          <li>
            <a href="/vqmt/">VQMT</a>
            <ul>
              <li><a href="/vqmt/plugins/">VQMT Plugins</a></li>
            </ul>
          </li>
          <li>
            <a href="/stereo_quality/">Video Quality Measurement Tool 3D</a>
            <ul>
              <li><a href="/stereo_quality/correction/">Stereo Artifacts Correction</a></li>
              <li><a href="/stereo_quality/metrics/">Stereo Quality Metrics</a></li>
              <li><a href="/stereo_quality/reports/">All Stereo Quality Reports</a></li>
            </ul>
          </li>
          <li>
            <a href="/datasets/">MSU Datasets Collection</a>
            <ul>
            </ul>
          </li>
          <li>
            <a href="/metrics/">Metrics Research</a>
            <ul>
            </ul>
          </li>
          <li>
            <a href="/video_filters/">Video Filters</a>
            <ul>
              <li><a href="/video_filters/image/">Image Processing Filters</a></li>
              <li><a href="/video_filters/public/">Free Filters</a></li>
              <li><a href="/video_filters/virtualdub/">VirtualDub Filters</a></li>
            </ul>
          </li>
          <li>
            <a href="/other/">Other Projects</a>
            <ul>
            </ul>
          </li>
        </ul>
      </div>
    </div>
    <div class="footer">
      <div class="footer-column copyright">
        <img alt="MSU Graphics & Multimedia Lab Video Group" class="logo" src="/assets/img/logo.svg">
        <div class="text">
          MSU Graphics & Media Lab Video Group
          <br>
          2019&ndash;2022
        </div>
      </div>
      <div class="footer-column">
        <ul>
          <li><a href="/codecs/">Codecs Comparisons & Optimization</a></li>
          <li><a href="/vqmt/">VQMT</a></li>
          <li><a href="/stereo_quality/">Video Quality Measurement Tool 3D</a></li>
          <li><a href="/datasets/">MSU Datasets Collection</a></li>
          <li><a href="/metrics/">Metrics Research</a></li>
          <li><a href="/video_filters/">Video Filters</a></li>
          <li><a href="/other/">Other Projects</a></li>
        </ul>
      </div>
      <div class="footer-column">
        <ul>
          <li><a href="/about/">About Us</a></li>
          <li><a href="/benchmarks/">Benchmarks</a></li>
          <li><a href="/projects/">Projects</a></li>
          <li><a href="/publications/">Publications</a></li>
          <li><a href="/contacts/">Contacts</a></li>
        </ul>
      </div>
    </div>
  </body>
</html>